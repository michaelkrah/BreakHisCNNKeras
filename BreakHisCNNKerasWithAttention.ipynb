{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXEhyXZyl7mp"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Rescaling, RandomFlip, RandomRotation\n",
        "from tensorflow.keras.metrics import Precision, Recall, AUC\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To import datasets from google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "Gt3UdGcmr2o_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o gdrive/MyDrive/BU/BreaKHis_v1.zip"
      ],
      "metadata": {
        "id": "nQrW38KAr1hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "image_size = (700, 460)\n",
        "batch_size = 32\n",
        "# Code from https://keras.io/examples/vision/image_classification_from_scratch/\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"BreaKHis_v1/BreaKHis_v1/histology_slides/data\",\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    \"BreaKHis_v1/BreaKHis_v1/histology_slides/data\",\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=1337,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size,\n",
        ")\n"
      ],
      "metadata": {
        "id": "Nghl5O2Sr5fB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Rescaling, RandomFlip, RandomRotation\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "lr_scheduler = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.1,\n",
        "    patience=10,\n",
        "    min_lr=0.00001\n",
        ")\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=15,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Attention layer code adapted from https://www.kaggle.com/code/haithemhermessi/attention-mechanism-keras-as-simple-as-possible\n",
        "\n",
        "class AttentionLayer(layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.attention_weights = self.add_weight(\n",
        "            shape=(input_shape[-1], 1),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        attention_score = tf.matmul(inputs, self.attention_weights)\n",
        "        attention_weights = tf.nn.softmax(attention_score, axis=1)\n",
        "        weighted_output = inputs * attention_weights\n",
        "        return tf.reduce_sum(weighted_output, axis=1)\n",
        "\n",
        "input_shape = (700, 460, 3)\n",
        "model_with_attention = Sequential(\n",
        "    [\n",
        "        keras.Input(shape=input_shape),\n",
        "        Rescaling(scale=1./255),\n",
        "\n",
        "\n",
        "        Conv2D(32, kernel_size=(3,3), activation=\"relu\"),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "        Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        Conv2D(256, kernel_size=(3, 3), activation=\"relu\"),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Dropout(0.2),\n",
        "\n",
        "\n",
        "        Flatten(),\n",
        "        Dropout(0.5),\n",
        "        layers.Dense(128, activation=\"relu\"),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(64, activation=\"relu\"),\n",
        "        Dense(1, activation=\"sigmoid\"),\n",
        "    ]\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "-dSg9bZaKJ4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_attention.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy'],\n",
        ")"
      ],
      "metadata": {
        "id": "JK1IZShLLrZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model_with_attention.fit(\n",
        "    train_ds,\n",
        "    epochs=40,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=[lr_scheduler, early_stopping]\n",
        ")\n"
      ],
      "metadata": {
        "id": "oBPILKY0sK4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_val_accuracy = history.history['val_accuracy'][-1]\n",
        "print(final_val_accuracy)"
      ],
      "metadata": {
        "id": "84sGgsnYsM7s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}